{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine tuning for named entity recognition",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Qnl_oPPk66-z"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XwvfVMH69Vr"
      },
      "source": [
        "!wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u9Upt4W7AlI"
      },
      "source": [
        "# Read a CoNLL file\n",
        "\n",
        "def read_conll(file_path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    raw_text = file_path.read_text().strip()\n",
        "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for line in doc.split('\\n'):\n",
        "            token, tag = line.split('\\t')\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs\n",
        "\n",
        "texts, tags = read_conll('wnut17train.conll')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcrDv1zS7GC7"
      },
      "source": [
        "# Encode tag and texts\n",
        "\n",
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnJreQP17Ih_"
      },
      "source": [
        "# Train test split\n",
        "\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKp3-Dze7TIs"
      },
      "source": [
        "# Tokenize text\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0qX0_2t7WL6"
      },
      "source": [
        "# Handle out-of-vocabulary words by only training on the tag labels for the first subtoken of a split token and setting the labels we wish to ignore to -100\n",
        "\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkJvtFkg7mMh"
      },
      "source": [
        "# Dataset Class\n",
        "\n",
        "class WNUTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
        "val_dataset = WNUTDataset(val_encodings, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CozQh-0B7mZp"
      },
      "source": [
        "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUDWS7n_7qHu"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "idx = 0\n",
        "model_path = f'gdrive/MyDrive/model_{idx}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,          # output directory\n",
        "    num_train_epochs=1, \n",
        "    evaluation_strategy=\"epoch\"             # total number of training epochs\n",
        ")\n",
        "\n",
        "# Trainer object \n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         \n",
        "    args=training_args,                 \n",
        "    train_dataset=train_dataset,        \n",
        "    eval_dataset=val_dataset             \n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FeDj6iT7tjM"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_c3zp3e7z_q"
      },
      "source": [
        "!mkdir here\n",
        "trainer.save_model(\"here\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}